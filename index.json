[{"authors":null,"categories":null,"content":"I am a fourth year PhD student in the Machine Learning program at Northeastern University, advised by Professor Jan-Willem van de Meent. I got my BSc in Artificial Intelligence and Computer Science at the University of Edinburgh. After that, I did an MSc in Data Science at the same University.\nI am interested in deep generative models and how we can guide them towards learning better representations. For similar reasons, I am also interested in representation leaning, particularly the approaches that are inspired by information theory. I am also a fan of probabilistic programming which provides exciting opportunities for abstracting probabilistic models, as well as improving efficiency in inference.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://babak0032.github.io/author/babak-esmaeili/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/babak-esmaeili/","section":"authors","summary":"I am a fourth year PhD student in the Machine Learning program at Northeastern University, advised by Professor Jan-Willem van de Meent. I got my BSc in Artificial Intelligence and Computer Science at the University of Edinburgh.","tags":null,"title":"Babak Esmaeili","type":"authors"},{"authors":["Alican Bozkurt","Babak Esmaeili","Dana H. Brooks","Jennifer G. Dy","Jan-Willem van de Meent"],"categories":null,"content":"","date":1594080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594080000,"objectID":"da231f62c5d80d6f6e64df46ee8bb324","permalink":"https://babak0032.github.io/publication/bozkurt-2019-evaluating/","publishdate":"2020-07-07T00:00:00Z","relpermalink":"/publication/bozkurt-2019-evaluating/","section":"publication","summary":"Variational autoencoders (VAEs) optimize an objective that comprises a reconstruction loss (the distortion) and a KL term (the rate). The rate is an upper bound on the mutual information, which is often interpreted as a regularizer that controls the degree of compression. We here examine whether inclusion of the rate term also improves generalization. We perform rate-distortion analyses in which we control the strength of the rate term, the network capacity, and the difficulty of the generalization problem. Lowering the strength of the rate term paradoxically improves generalization in most settings, and reducing the mutual information typically leads to underfitting. Moreover, we show that generalization performance continues to improve even after the mutual information saturates, indicating that the gap on the bound (i.e. the KL divergence relative to the inference marginal) affects generalization. This suggests that the standard spherical Gaussian prior is not an inductive bias that typically improves generalization, prompting further work to understand what choices of priors improve generalization in VAEs.","tags":null,"title":"Rate-Regularization and Generalization in VAEs","type":"publication"},{"authors":["Alican Bozkurt","Babak Esmaeili","Dana H. Brooks","Jennifer G. Dy","Jan-Willem van de Meent"],"categories":null,"content":"","date":1546387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546387200,"objectID":"34095551d9b7d69350157dee68791485","permalink":"https://babak0032.github.io/publication/bozkurt-2018-can/","publishdate":"2019-01-02T00:00:00Z","relpermalink":"/publication/bozkurt-2018-can/","section":"publication","summary":"An implicit goal in works on deep generative models is that such models should be able to generate novel examples that were not previously seen in the training data. In this paper, we investigate to what extent this property holds for widely employed variational autoencoder (VAE) architectures. VAEs maximize a lower bound on the log marginal likelihood, which implies that they will in principle overfit the training data when provided with a sufficiently expressive decoder. In the limit of an infinite capacity decoder, the optimal generative model is a uniform mixture over the training data. More generally, an optimal decoder should output a weighted average over the examples in the training data, where the magnitude of the weights is determined by the proximity in the latent space. This leads to the hypothesis that, for a sufficiently high capacity encoder and decoder, the VAE decoder will perform nearest-neighbor matching according to the coordinates in the latent space. To test this hypothesis, we investigate generalization on the MNIST dataset. We consider both generalization to new examples of previously seen classes, and generalization to the classes that were withheld from the training set. In both cases, we find that reconstructions are closely approximated by nearest neighbors for higher-dimensional parameterizations. When generalizing to unseen classes however, lower-dimensional parameterizations offer a clear advantage.","tags":["NeurIPS Workshop on Critiquing and Correcting Trends in Machine Learning"],"title":"Can VAEs Generate Novel Examples?","type":"publication"},{"authors":["Babak Esmaeili","Hao Wu","Sarthak Jain","Alican Bozkurt","Narayanaswamy Siddharth","Brooks Paige","Dana H Brooks","Jennifer Dy","Jan-Willem van de Meent"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"a099646d19e25d6d2ee9b33b0fee7560","permalink":"https://babak0032.github.io/publication/esmaeili-2019-structureda/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/esmaeili-2019-structureda/","section":"publication","summary":"Deep latent-variable models learn representations of high-dimensional data in an unsupervised manner. A number of recent efforts have focused on learning representations that disentangle statistically independent axes of variation by introducing modifications to the standard objective function. These approaches generally assume a simple diagonal Gaussian prior and as a result are not able to reliably disentangle discrete factors of variation. We propose a two-level hierarchical objective to control relative degree of statistical independence between blocks of variables and individual variables within blocks. We derive this objective as a generalization of the evidence lower bound, which allows us to explicitly represent the trade-offs between mutual information between data and representation, KL divergence between representation and prior, and coverage of the support of the empirical data distribution. Experiments on a variety of datasets demonstrate that our objective can not only disentangle discrete variables, but that doing so also improves disentanglement of other variables and, importantly, generalization even to unseen combinations of factors. ","tags":["AISTATS"],"title":"Structured Disentangled Representations","type":"publication"},{"authors":["Babak Esmaeili","Hongyi Huang","Byron C. Wallace","Jan-Willem van de Meent"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"a414d33d3380293d8b7c13fea0f9e125","permalink":"https://babak0032.github.io/publication/esmaeili-2019-structuredb/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/esmaeili-2019-structuredb/","section":"publication","summary":"We present Variational Aspect-based Latent Topic Allocation (VALTA), a family of autoencoding topic models that learn aspect-based representations of reviews. VALTA defines a user-item encoder that maps bag-of-words vectors for combined reviews associated with each paired user and item onto structured embeddings, which in turn define per-aspect topic weights. We model individual reviews in a structured manner by infer- ring an aspect assignment for each sentence in a given review, where the per-aspect topic weights obtained by the user-item encoder serve to define a mixture over topics, conditioned on the aspect. The result is an autoencoding neural topic model for reviews, which can be trained in a fully unsupervised manner to learn topics that are structured into aspects.","tags":["AISTATS"],"title":"Structured Neural Topic Models for Reviews","type":"publication"}]